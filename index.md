---
title:
layout: page
feature_image: "https://raw.githubusercontent.com/erichson/sciml/master/img/logo.png"
feature_text:
---

### 1st Workshop on Scientific-Driven Deep Learning (SciDL)
***
##### 8:00-14:30 (PST) on Wednesday July 1, 2020 
##### Location: Zoom
##### Register here: [https://forms.gle/o1WCSBTbeoahj5ww8](https://forms.gle/o1WCSBTbeoahj5ww8)
***
<p style='text-align: justify;'> Deep learning is playing a growing role in the area of fluid dynamics, climate science and in many other scientific disciplines. Classically, deep learning has focused on an model agnostic learning approaches ignoring any prior knowledge that is known about the problem under consideration. However, limited data can severely challenge our ability to train complex and deep models for scientific applications. This workshop focuses on scientific-driven deep learning to explore challenges and solutions for more robust and interpretable learning.
 </p>

<div class="embeddable_schedule" shortname="SciDL" daterange="future" sitefooter></div>
<script src="https://researchseminars.org/embed_seminars.js" onload="seminarEmbedder.initialize({'addCSS': true});"></script>


##### Key Note Speakers
* [George Em Karniadakis](https://www.brown.edu/research/projects/crunch/george-karniadakis) (Brown University) [paper](https://arxiv.org/abs/1910.03193) (video)[]
- Title: DeepOnet: Learning nonlinear operators based on the universal approximation theorem of operators
- Abstract: It is widely known that neural networks (NNs) are universal approximators of continuous functions, however, a less known but powerful result is that a NN with a single hidden layer can approximate accurately any nonlinear continuous operator. This universal approximation theorem of operators is suggestive of the potential of NNs in learning from scattered data any continuous operator or complex system. To realize this theorem, we design a new NN with small generalization error, the deep operator network (DeepONet), consisting of a NN for encoding the discrete input function space (branch net) and another NN for encoding the domain of the output functions (trunk net). We demonstrate that DeepONet can learn various explicit operators, e.g., integrals and fractional Laplacians, as well as implicit operators that represent deterministic and stochastic differential equations. We study, in particular, different formulations of the input function space and its effect on the generalization error.

<iframe width="560" height="315" src="https://www.youtube.com/embed/6WVRZt9_pWM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

* [Michael P. Brenner](https://www.seas.harvard.edu/brenner/Home.html) (Harvard University)


##### Invited Speakers
* [Frank Noe](http://www.mi.fu-berlin.de/en/math/groups/comp-mol-bio/) (FU Berlin)
* [Lars Ruthotto](http://www.mathcs.emory.edu/~lruthot/) (Emory University)
* [Yasaman Bahri](https://yasamanb.github.io/) (Google Brain)
* [Omri Azencot](http://omriazencot.com/) (UCLA)
* [Michael Muehlebach](https://sites.google.com/view/mmuehlebach) (UC Berkeley)
* [Alejandro Queiruga](https://afqueiruga.github.io/) (Google, LLC)
* [Elizabeth Qian](https://sites.google.com/view/elizabeth-qian/) (MIT)

##### Organizers 
* [N. Benjamin Erichson](https://www.benerichson.com/) (UC Berkeley)
* [Michael W. Mahoney](https://www.stat.berkeley.edu/~mmahoney/) (UC Berkeley)
* [Tess Smidt](https://crd.lbl.gov/departments/computational-science/ccmc/staff/alvarez-fellows/tess-smidt/) (LBL)
* [Steven L. Brunton](https://www.eigensteve.com/) (University of Washington)
* [J. Nathan Kutz](https://faculty.washington.edu/kutz/) (University of Washington)

